{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetaflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     FlowSpec,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Parameter,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     card,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129;43m@project\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhotel_reservations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;43;03m# @pypi_base()\u001b[39;49;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTraining\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mFlowSpec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43;03m    Training Pipeline.\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;43;03m    or not.\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mTraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@project\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotel_reservations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# @pypi_base()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTraining\u001b[39;00m(FlowSpec):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Training Pipeline.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    or not.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m((Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;129m@card\u001b[39m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;129m@step\u001b[39m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m        Start and prepare the training pipeline\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import mlflow\n",
    "import joblib\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from mlflow.models import infer_signature, infer_pip_requirements\n",
    "from metaflow import (\n",
    "    FlowSpec,\n",
    "    # Parameter,\n",
    "    step,\n",
    "    # pypi_base,\n",
    "    project,\n",
    "    current,\n",
    "    card,\n",
    ")\n",
    "from pipelines.common import load_dataset\n",
    "\n",
    "\n",
    "@project(name=\"hotel_reservations\")\n",
    "# @pypi_base()\n",
    "class Training(FlowSpec):\n",
    "    \"\"\"\n",
    "    Training Pipeline.\n",
    "\n",
    "    This pipline trains, evaluates and registers a model\n",
    "    to predict if a user will book an hotel from a list\n",
    "    or not.\n",
    "    \"\"\"\n",
    "\n",
    "    data_path = \"/Users/toluwaniosabiya/Desktop/hotel_reservations/data/case_study_dataset.parquet\"\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start and prepare the training pipeline\n",
    "        \"\"\"\n",
    "        print(self.data_path)\n",
    "\n",
    "        self.df = load_dataset(self.data_path)\n",
    "        # self.df = self.df[:500]\n",
    "\n",
    "        dotenv.load_dotenv()\n",
    "        self.mlflow_tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "        try:\n",
    "            # Start a new mlflow run to track the experiment\n",
    "            # using the same run_id from metaflow\n",
    "            # for easy tracking across platforms\n",
    "\n",
    "            run = mlflow.start_run(run_name=current.run_id)\n",
    "            self.mlflow_run_id = run.info.run_id\n",
    "        except Exception as e:\n",
    "            message = f\"Failed to connect to MLFlow server: \\\n",
    "                {self.mlflow_tracking_uri}\"\n",
    "            raise RuntimeError(message) from e\n",
    "\n",
    "        self.next(self.transform)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Preprocess and transform dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # Split into training and test sets based on\n",
    "        # unique indices\n",
    "        unique_search_ids = self.df[\"searchId\"].unique()\n",
    "        train_ids, test_ids = train_test_split(\n",
    "            unique_search_ids, test_size=0.15, random_state=42\n",
    "        )\n",
    "\n",
    "        from pipelines.common import HotelBooking\n",
    "\n",
    "        self.data_transformer = HotelBooking()\n",
    "\n",
    "        self.df_train = self.data_transformer.fit_transform(\n",
    "            self.df[self.df[\"searchId\"].isin(train_ids)]\n",
    "        )\n",
    "        self.df_test = self.data_transformer.transform(\n",
    "            self.df[self.df[\"searchId\"].isin(test_ids)]\n",
    "        )\n",
    "\n",
    "        mapper = self.data_transformer.encoding_map\n",
    "        cat_cols = [key for key in mapper.keys()]\n",
    "\n",
    "        for column in cat_cols:\n",
    "            print(self.df_test[f\"{column}_encoded\"].isna().sum())\n",
    "\n",
    "        self.next(self.split)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def split(self):\n",
    "        \"\"\"\n",
    "        Split data into training and test sets\n",
    "        \"\"\"\n",
    "        from pipelines.common import distinguish_label_and_features\n",
    "\n",
    "        self.X_train, self.y_train = distinguish_label_and_features(self.df_train)\n",
    "        self.X_test, self.y_test = distinguish_label_and_features(self.df_test)\n",
    "\n",
    "        print(self.X_train.columns)\n",
    "        print(self.X_test.columns)\n",
    "        print(self.y_train.name)\n",
    "        print(self.y_test.name)\n",
    "\n",
    "        self.next(self.train)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train model and log the training parameters\n",
    "        \"\"\"\n",
    "        mlflow.set_tracking_uri(self.mlflow_tracking_uri)\n",
    "        with mlflow.start_run(run_id=self.mlflow_run_id):\n",
    "            mlflow.autolog()\n",
    "\n",
    "            self.scale_pos_weight = sum(self.y_train == 0) / sum(self.y_train == 1)\n",
    "            # The above controls the balance of positive and negative weights\n",
    "\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                scale_pos_weight=self.scale_pos_weight,\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            self.param_grid = {\n",
    "                \"n_estimators\": [100, 200, 500],\n",
    "                \"max_depth\": [3, 5, 7],\n",
    "                \"learning_rate\": [0.01, 0.1, 1],\n",
    "                \"min_child_weight\": [1, 3, 5],\n",
    "            }\n",
    "\n",
    "            # Set up the GridSearchCV\n",
    "            grid_search_xgb = GridSearchCV(\n",
    "                estimator=xgb_model,\n",
    "                param_grid=self.param_grid,\n",
    "                scoring=\"recall\",  # Recall is chosen because correctly identifying the positive class is desirable\n",
    "                cv=5,\n",
    "                verbose=2,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "\n",
    "            grid_search_xgb.fit(self.X_train, self.y_train)\n",
    "            print(\"Best Parameters:\", grid_search_xgb.best_params_)\n",
    "            self.model = grid_search_xgb.best_estimator_\n",
    "\n",
    "        self.next(self.evaluate)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate and log metrics\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        pd.DataFrame(self.y_test, y_pred).to_csv(\"results.csv\")\n",
    "        self.report = classification_report(\n",
    "            self.y_test, y_pred, target_names=[\"Not Booked\", \"Booked\"], output_dict=True\n",
    "        )\n",
    "        self.conf_matrix = confusion_matrix(self.y_test, y_pred)\n",
    "\n",
    "        self.pos_class_recall = round(self.report[\"Booked\"][\"recall\"], 2)\n",
    "        self.mean_recall = round(\n",
    "            np.mean(\n",
    "                (self.report[\"Booked\"][\"recall\"], self.report[\"Not Booked\"][\"recall\"])\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        mlflow.set_tracking_uri(self.mlflow_tracking_uri)\n",
    "        with mlflow.start_run(run_id=self.mlflow_run_id):\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"pos_class_recall\": self.pos_class_recall,\n",
    "                    \"mean_recall\": self.mean_recall,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        self.next(self.register_model)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def register_model(self):\n",
    "        pass\n",
    "        self.next(self.preview)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def preview(self):\n",
    "        pass\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"End the pipeline\"\"\"\n",
    "\n",
    "    def _get_model_artifacts(self, directory: str):\n",
    "        \"\"\"\n",
    "        Return the list of artifacts that will be included in the model.\n",
    "\n",
    "        This will include the functions to transform raw inputs into a\n",
    "        format acceptable by the model.\n",
    "        \"\"\"\n",
    "        # Save the model\n",
    "        model_path = (Path(directory) / \"hotel_reservations.json\").as_posix()\n",
    "        self.model.save_model(model_path)\n",
    "\n",
    "        # Save the transformer function\n",
    "        transformer_path = (Path(directory) / \"transformer.joblib\").as_posix()\n",
    "        joblib.dump(self.data_transformer, transformer_path)\n",
    "\n",
    "        return {\n",
    "            \"model\": model_path,\n",
    "            \"data_transformer\": transformer_path,\n",
    "        }\n",
    "\n",
    "    def _get_model_signature(self):\n",
    "        \"\"\"\n",
    "        Return the model's signature\n",
    "\n",
    "        This will include the expected format for model inputs and outputs.\n",
    "        It gives some information about the correct use of the model.\n",
    "        \"\"\"\n",
    "        input_dict = self.df[:1].drop(columns=\"bookingLabel\").transpose().to_dict()[0]\n",
    "        model_input = {k: [v] for k, v in input_dict.items()}\n",
    "\n",
    "        model_output = {\n",
    "            \"hotelId\": model_input[\"hotelId\"],\n",
    "            \"prediction(s)\": [1],\n",
    "            \"prediction_probability(s)\": [0.708354],\n",
    "        }\n",
    "\n",
    "        params = {\"data_capture\": False}\n",
    "\n",
    "        return infer_signature(\n",
    "            model_input=model_input, model_output=model_output, params=params\n",
    "        )\n",
    "\n",
    "    def _get_model_pip_requirements(self):\n",
    "        \"\"\"\n",
    "        Return list of required packages to run model\n",
    "        \"\"\"\n",
    "\n",
    "        with open(Path.cwd().parent / \"requirements.txt\", \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            # Remove comments and blank lines, and strip whitespace\n",
    "            requirements = [line.strip() for line in lines]\n",
    "\n",
    "        requirements = [item for item in requirements if not item.startswith(\"#\")]\n",
    "\n",
    "        return requirements\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
